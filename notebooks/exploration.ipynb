{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploration Notebook\n",
        "Scratchpad to explore the Free NBA API and DuckDB schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b7ea07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# heartbeat\n",
        "print(\"Kernel is alive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de822639",
      "metadata": {},
      "source": [
        "# Sanity check API call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8bb1fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import httpx, os, json\n",
        "\n",
        "API_KEY = os.getenv(\"BALLDONTLIE_API_KEY\")\n",
        "BASE = \"https://api.balldontlie.io/v1\"\n",
        "HEADERS = {\"Authorization\": API_KEY}\n",
        "\n",
        "date_str = \"2025-06-22\"\n",
        "\n",
        "with httpx.Client(timeout=httpx.Timeout(10.0, connect=5.0)) as client:\n",
        "    r = client.get(f\"{BASE}/games\", headers=HEADERS, params={\"dates[]\": date_str, \"per_page\": 100, \"page\": 1})\n",
        "    print(\"HTTP\", r.status_code)\n",
        "    # If it’s JSON, show a quick summary; otherwise show text\n",
        "    try:\n",
        "        payload = r.json()\n",
        "        print(\"keys:\", list(payload.keys()))\n",
        "        print(\"count in this page:\", len(payload.get(\"data\", [])))\n",
        "        print(\"meta:\", payload.get(\"meta\"))\n",
        "    except Exception:\n",
        "        print(r.text[:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c1ca71c",
      "metadata": {},
      "source": [
        "# 1. Get data back from the API for games on a given date e.g. 2025-04-27\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d417cafd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, time\n",
        "\n",
        "def fetch_games_on(date_str: str, per_page: int = 100) -> list[dict]:\n",
        "    all_rows, page = [], 1\n",
        "    with httpx.Client(timeout=httpx.Timeout(10.0, connect=5.0)) as client:\n",
        "        while True:\n",
        "            r = client.get(f\"{BASE}/games\", headers=HEADERS,\n",
        "                           params={\"dates[]\": date_str, \"per_page\": per_page, \"page\": page})\n",
        "            r.raise_for_status()\n",
        "            payload = r.json()\n",
        "            data = payload.get(\"data\", [])\n",
        "            meta = payload.get(\"meta\", {}) or {}\n",
        "            all_rows.extend(data)\n",
        "            if page >= (meta.get(\"total_pages\") or 1) or not data:\n",
        "                break\n",
        "            page += 1\n",
        "    return all_rows\n",
        "\n",
        "def simplify(g):\n",
        "    return {\n",
        "        \"game_id\": g[\"id\"],\n",
        "        \"date\": g[\"date\"][:10] if g.get(\"date\") else None,\n",
        "        \"status\": g.get(\"status\"),\n",
        "        \"home_team\": g[\"home_team\"][\"full_name\"],\n",
        "        \"away_team\": g[\"visitor_team\"][\"full_name\"],\n",
        "        \"home_score\": g.get(\"home_team_score\"),\n",
        "        \"away_score\": g.get(\"visitor_team_score\"),\n",
        "        \"season\": g.get(\"season\"),\n",
        "        \"postseason\": g.get(\"postseason\"),\n",
        "    }\n",
        "\n",
        "target_date = \"2025-04-27\"\n",
        "rows = fetch_games_on(target_date)\n",
        "print(f\"Found {len(rows)} games on {target_date}\")\n",
        "\n",
        "df = pd.DataFrame([simplify(g) for g in rows])\n",
        "df.sort_values([\"date\", \"game_id\"], inplace=True, ignore_index=True)\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1aeb0f8",
      "metadata": {},
      "source": [
        "# 2. Returns data just for a single game specified by gameID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d5e3e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import httpx, pandas as pd\n",
        "\n",
        "# Re-use your API key and BASE/HEADERS defined earlier\n",
        "API_KEY = \"<your_api_key>\"\n",
        "BASE = \"https://api.balldontlie.io/v1\"\n",
        "HEADERS = {\"Authorization\": API_KEY}\n",
        "\n",
        "def fetch_game(game_id: int) -> dict:\n",
        "    \"\"\"Fetch details for a single game by ID.\"\"\"\n",
        "    with httpx.Client(timeout=httpx.Timeout(10.0, connect=5.0)) as client:\n",
        "        r = client.get(f\"{BASE}/games/{game_id}\", headers=HEADERS)\n",
        "        r.raise_for_status()\n",
        "        return r.json()\n",
        "\n",
        "def simplify_game(resp: dict) -> dict:\n",
        "    \"\"\"Extract key fields from the raw JSON (unwrap 'data' if present).\"\"\"\n",
        "    # unravel the \"data\" block aka flatten it\n",
        "    g = resp.get(\"data\", resp)  # handle both raw object and wrapped\n",
        "    return {\n",
        "        \"game_id\": g[\"id\"],\n",
        "        \"date\": g.get(\"date\", \"\")[:10],\n",
        "        \"status\": g.get(\"status\"),\n",
        "        \"season\": g.get(\"season\"),\n",
        "        \"postseason\": g.get(\"postseason\"),\n",
        "        \"home_team\": g[\"home_team\"][\"full_name\"],\n",
        "        \"away_team\": g[\"visitor_team\"][\"full_name\"],\n",
        "        \"home_score\": g.get(\"home_team_score\"),\n",
        "        \"away_score\": g.get(\"visitor_team_score\"),\n",
        "    }\n",
        "\n",
        "\n",
        "# Example: Orlando Magic vs Boston Celtics on 2025-04-27\n",
        "example_id = 18422304\n",
        "game = fetch_game(example_id)\n",
        "print(\"Raw keys:\", list(game.keys()))\n",
        "\n",
        "row = simplify_game(game)\n",
        "df = pd.DataFrame([row])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b82d5308",
      "metadata": {},
      "source": [
        "# Player totals (points) for a single game — BallDontLie /stats?game_ids[]=<id>\n",
        "Gets a 401 as this requires a paid subscription"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07787ac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time\n",
        "import httpx\n",
        "import pandas as pd\n",
        "\n",
        "# Reuse existing globals if present; otherwise define them here\n",
        "try:\n",
        "    API_KEY\n",
        "    BASE\n",
        "    HEADERS\n",
        "except NameError:\n",
        "    API_KEY = \"<your_api_key>\"  # or os.getenv(\"BALLDONTLIE_API_KEY\")\n",
        "    BASE = \"https://api.balldontlie.io/v1\"\n",
        "    HEADERS = {\"Authorization\": API_KEY}    \n",
        "\n",
        "def fetch_player_stats_for_game(game_id: int, per_page: int = 100) -> list[dict]:\n",
        "    \"\"\"Fetch all player stat rows for a game, following pagination.\"\"\"\n",
        "    all_rows, page = [], 1\n",
        "    with httpx.Client(timeout=httpx.Timeout(10.0, connect=5.0)) as client:\n",
        "        while True:\n",
        "            r = client.get(\n",
        "                f\"{BASE}/stats\",\n",
        "                headers=HEADERS,\n",
        "                params={\"game_ids[]\": game_id, \"per_page\": per_page, \"page\": page},\n",
        "            )\n",
        "            r.raise_for_status()\n",
        "            payload = r.json()\n",
        "            data = payload.get(\"data\", payload)  # handle wrapped or bare\n",
        "            meta = payload.get(\"meta\", {}) or {}\n",
        "            if isinstance(data, dict):\n",
        "                # some rare cases return a single object\n",
        "                data = [data]\n",
        "            all_rows.extend(data)\n",
        "            if page >= (meta.get(\"total_pages\") or 1) or not data:\n",
        "                break\n",
        "            page += 1\n",
        "    return all_rows\n",
        "\n",
        "def player_points_dataframe(game_id: int) -> pd.DataFrame:\n",
        "    \"\"\"Aggregate total points per player for the game (exclude zero-point players).\"\"\"\n",
        "    rows = fetch_player_stats_for_game(game_id)\n",
        "    # Build tidy rows\n",
        "    tidy = []\n",
        "    for r in rows:\n",
        "        player = r.get(\"player\") or {}\n",
        "        team = r.get(\"team\") or {}\n",
        "        tidy.append({\n",
        "            \"player_id\": player.get(\"id\"),\n",
        "            \"player_name\": f\"{player.get('first_name','')} {player.get('last_name','')}\".strip(),\n",
        "            \"team_id\": team.get(\"id\"),\n",
        "            \"team_abbr\": team.get(\"abbreviation\"),\n",
        "            \"pts\": r.get(\"pts\") or 0,\n",
        "        })\n",
        "    df = pd.DataFrame(tidy)\n",
        "    if df.empty:\n",
        "        return df\n",
        "    # Sum points per player, drop zero scorers, sort desc\n",
        "    agg = (\n",
        "        df.groupby([\"player_id\", \"player_name\", \"team_id\", \"team_abbr\"], dropna=False)[\"pts\"]\n",
        "          .sum()\n",
        "          .reset_index(name=\"points_total\")\n",
        "    )\n",
        "    agg = agg[agg[\"points_total\"] > 0].sort_values([\"points_total\", \"player_name\"], ascending=[False, True]).reset_index(drop=True)\n",
        "    return agg\n",
        "\n",
        "# ==== Example usage ====\n",
        "game_id = 18422304  # e.g., 2025-04-27 ORL vs BOS\n",
        "player_points_df = player_points_dataframe(game_id)\n",
        "print(f\"Fetched {len(player_points_df)} scoring players for game_id={game_id}\")\n",
        "player_points_df.head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "638eed1e",
      "metadata": {},
      "source": [
        "# Get a mapping of the teams to use in the next steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6830f939",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install requests python-dateutil\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "API_BASE = \"https://api.balldontlie.io/v1\"\n",
        "API_KEY = os.getenv(\"BALLDONTLIE_API_KEY\", \"<your_api_key>\")\n",
        "HEADERS = {\"Authorization\": API_KEY}\n",
        "\n",
        "# Simple in-memory cache\n",
        "_TEAM_CACHE = {\n",
        "    \"fetched_at\": 0.0,\n",
        "    \"ttl_sec\": 24 * 60 * 60,   # 24h\n",
        "    \"teams\": [],               # raw team objects\n",
        "    \"index\": {},               # str -> team_id (for quick lookup)\n",
        "    \"reverse\": {}              # team_id -> canonical name\n",
        "}\n",
        "\n",
        "def _normalise(s: str) -> str:\n",
        "    return \" \".join(s.strip().lower().split())\n",
        "\n",
        "def _build_index(teams: List[dict]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    \"\"\"\n",
        "    Build a flexible lookup index:\n",
        "      - full_name (e.g., \"Boston Celtics\")\n",
        "      - name (e.g., \"Celtics\")\n",
        "      - abbreviation (e.g., \"BOS\")\n",
        "      - city + name (e.g., \"Boston Celtics\")\n",
        "    \"\"\"\n",
        "    idx: Dict[str, int] = {}\n",
        "    rev: Dict[int, str] = {}\n",
        "\n",
        "    for t in teams:\n",
        "        tid = t[\"id\"]\n",
        "        abbr = t.get(\"abbreviation\", \"\") or \"\"\n",
        "        city = t.get(\"city\", \"\") or \"\"\n",
        "        name = t.get(\"name\", \"\") or \"\"\n",
        "        full = t.get(\"full_name\", \"\") or f\"{city} {name}\".strip()\n",
        "\n",
        "        rev[tid] = full\n",
        "\n",
        "        candidates = {\n",
        "            _normalise(full),\n",
        "            _normalise(name),\n",
        "            _normalise(abbr),\n",
        "            _normalise(f\"{city} {name}\"),\n",
        "            _normalise(city),  # sometimes people type just the city\n",
        "        }\n",
        "\n",
        "        for key in candidates:\n",
        "            if not key:\n",
        "                continue\n",
        "            # If collision (e.g., \"Los Angeles\") occurs, keep it ambiguous for now.\n",
        "            # We'll handle ambiguity at query time by collecting candidates.\n",
        "            if key in idx and idx[key] != tid:\n",
        "                # mark as ambiguous sentinel by storing -1\n",
        "                idx[key] = -1\n",
        "            else:\n",
        "                idx[key] = tid\n",
        "\n",
        "    return idx, rev\n",
        "\n",
        "def _fetch_teams() -> List[dict]:\n",
        "    resp = requests.get(f\"{API_BASE}/teams\", headers=HEADERS, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    payload = resp.json()\n",
        "    return payload.get(\"data\", [])\n",
        "\n",
        "def _ensure_team_cache(force: bool = False) -> None:\n",
        "    now = time.time()\n",
        "    if force or (now - _TEAM_CACHE[\"fetched_at\"] > _TEAM_CACHE[\"ttl_sec\"]) or not _TEAM_CACHE[\"teams\"]:\n",
        "        teams = _fetch_teams()\n",
        "        idx, rev = _build_index(teams)\n",
        "        _TEAM_CACHE.update({\n",
        "            \"fetched_at\": now,\n",
        "            \"teams\": teams,\n",
        "            \"index\": idx,\n",
        "            \"reverse\": rev\n",
        "        })\n",
        "\n",
        "def list_teams() -> List[Tuple[int, str, str]]:\n",
        "    \"\"\"Convenience helper for a human-readable menu.\"\"\"\n",
        "    _ensure_team_cache()\n",
        "    out = []\n",
        "    for t in _TEAM_CACHE[\"teams\"]:\n",
        "        out.append((t[\"id\"], t.get(\"full_name\", f\"{t.get('city','')} {t.get('name','')}\".strip()), t.get(\"abbreviation\",\"\")))\n",
        "    # sort nicely\n",
        "    return sorted(out, key=lambda x: x[1])\n",
        "\n",
        "def resolve_team(query: str) -> int:\n",
        "    \"\"\"\n",
        "    Resolve a team by a flexible string:\n",
        "      - \"Boston Celtics\" (full_name)\n",
        "      - \"Celtics\" (name)\n",
        "      - \"BOS\" (abbreviation)\n",
        "      - \"Boston\", \"Los Angeles\", etc. (may be ambiguous)\n",
        "    Handles ambiguity by attempting smart narrowing; otherwise raises ValueError with suggestions.\n",
        "    \"\"\"\n",
        "    _ensure_team_cache()\n",
        "    q = _normalise(query)\n",
        "    idx = _TEAM_CACHE[\"index\"]\n",
        "\n",
        "    # Direct hit?\n",
        "    if q in idx and idx[q] > 0:\n",
        "        return idx[q]\n",
        "\n",
        "    # If stored as ambiguous sentinel (-1) or not found, try substring search over full_name and city+name\n",
        "    candidates = []\n",
        "    for t in _TEAM_CACHE[\"teams\"]:\n",
        "        full = _normalise(t.get(\"full_name\", \"\"))\n",
        "        city_name = _normalise(f\"{t.get('city','')} {t.get('name','')}\")\n",
        "        abbr = _normalise(t.get(\"abbreviation\",\"\"))\n",
        "        name_only = _normalise(t.get(\"name\",\"\"))\n",
        "\n",
        "        if q == full or q == city_name or q == abbr or q == name_only:\n",
        "            candidates.append(t)\n",
        "        elif q in full or q in city_name:\n",
        "            candidates.append(t)\n",
        "\n",
        "    # Unique?\n",
        "    unique_ids = list({t[\"id\"] for t in candidates})\n",
        "    if len(unique_ids) == 1:\n",
        "        return unique_ids[0]\n",
        "\n",
        "    # No/Many → raise with helpful suggestions\n",
        "    if not candidates:\n",
        "        # Show a few closest by simple contains on tokens\n",
        "        hints = \", \".join([t[\"full_name\"] for t in _TEAM_CACHE[\"teams\"][:5]])\n",
        "        raise ValueError(f\"No team match for '{query}'. Example teams: {hints}\")\n",
        "    else:\n",
        "        opts = \", \".join(sorted({t[\"full_name\"] for t in candidates}))\n",
        "        raise ValueError(f\"Ambiguous team '{query}'. Did you mean one of: {opts}\")\n",
        "\n",
        "def team_name(team_id: int) -> Optional[str]:\n",
        "    _ensure_team_cache()\n",
        "    return _TEAM_CACHE[\"reverse\"].get(team_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d96e28a3",
      "metadata": {},
      "source": [
        "# Check cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b8df63",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Force refresh the cache (optional)\n",
        "_ensure_team_cache(force=True)\n",
        "\n",
        "# Peek at the whole cache dict\n",
        "# Raw team objects (first 2 only for display)\n",
        "# _TEAM_CACHE[\"teams\"][:2]\n",
        "\n",
        "# Lookup dictionary (sample keys)\n",
        "# list(_TEAM_CACHE[\"index\"].items())[:10]\n",
        "\n",
        "# Reverse lookup (id → name)\n",
        "_TEAM_CACHE[\"reverse\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575021bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Force refresh the cache (optional)\n",
        "_ensure_team_cache(force=True)\n",
        "\n",
        "# Peek at the whole cache dict\n",
        "_TEAM_CACHE.keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437c2f6a",
      "metadata": {},
      "source": [
        "# Points 3 and 4 on the brief\n",
        "## 3. Create an endpoint that allows one to download a CSV file with aggregated statistics by team\n",
        "for a requested date period, start_date, end_date, e.g., 2022-01-31\n",
        "## 4. The CSV file should contain: Team name, Median score, Percentage of games won. Additional\n",
        "fields are allowed. (Stretch goal: put in home and away summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d371923",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install requests pandas python-dateutil numpy\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dateutil.parser import isoparse\n",
        "\n",
        "API_BASE = \"https://api.balldontlie.io/v1\"\n",
        "API_KEY = os.getenv(\"BALLDONTLIE_API_KEY\", \"<your_api_key>\")  # <-- replace if not using env var\n",
        "HEADERS = {\"Authorization\": API_KEY}\n",
        "\n",
        "def fetch_games_for_team(team_id: int, start_date: str, end_date: str, per_page: int = 100, max_pages: int = 100):\n",
        "    \"\"\"\n",
        "    Fetch all games for a given team and date range (inclusive). Handles pagination.\n",
        "    Dates must be 'YYYY-MM-DD'.\n",
        "    \"\"\"\n",
        "    # Basic validation to avoid accidental reversed dates\n",
        "    sd = isoparse(start_date).date()\n",
        "    ed = isoparse(end_date).date()\n",
        "    if ed < sd:\n",
        "        raise ValueError(\"end_date must be >= start_date\")\n",
        "        \n",
        "    games = []\n",
        "    page = 1\n",
        "    while True:\n",
        "        params = {\n",
        "            \"team_ids[]\": team_id,\n",
        "            \"start_date\": start_date,\n",
        "            \"end_date\": end_date,\n",
        "            \"per_page\": per_page,\n",
        "            \"page\": page,\n",
        "        }\n",
        "        resp = requests.get(f\"{API_BASE}/games\", headers=HEADERS, params=params, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        payload = resp.json()\n",
        "        data = payload.get(\"data\", [])\n",
        "        meta = payload.get(\"meta\", {})\n",
        "        games.extend(data)\n",
        "        \n",
        "        total_pages = meta.get(\"total_pages\", 1)\n",
        "        if page >= total_pages or page >= max_pages:\n",
        "            break\n",
        "        page += 1\n",
        "        # be polite if rate-limited later\n",
        "        time.sleep(0.05)\n",
        "    return games\n",
        "\n",
        "def compute_team_aggregates(games: list, team_id: int):\n",
        "    if not games:\n",
        "        return {\n",
        "            \"team_id\": team_id,\n",
        "            \"team_name\": None,\n",
        "            \"games_count\": 0,\n",
        "            \"median_score\": None,\n",
        "            \"win_pct\": None,\n",
        "            \"home_games\": 0,\n",
        "            \"home_win_pct\": None,\n",
        "            \"away_games\": 0,\n",
        "            \"away_win_pct\": None,\n",
        "        }, pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    team_name = None\n",
        "\n",
        "    for g in games:\n",
        "        home = g[\"home_team\"]\n",
        "        away = g[\"visitor_team\"]\n",
        "        hs = g[\"home_team_score\"]\n",
        "        vs = g[\"visitor_team_score\"]\n",
        "        date = g[\"date\"]\n",
        "\n",
        "        if home[\"id\"] == team_id:\n",
        "            team_side = \"home\"\n",
        "            team_points, opp_points = hs, vs\n",
        "            opp_team = away.get(\"full_name\", away.get(\"name\"))\n",
        "            if team_name is None:\n",
        "                team_name = home.get(\"full_name\", home.get(\"name\"))\n",
        "        elif away[\"id\"] == team_id:\n",
        "            team_side = \"away\"\n",
        "            team_points, opp_points = vs, hs\n",
        "            opp_team = home.get(\"full_name\", home.get(\"name\"))\n",
        "            if team_name is None:\n",
        "                team_name = away.get(\"full_name\", away.get(\"name\"))\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        rows.append({\n",
        "            \"date\": date,\n",
        "            \"team_id\": team_id,\n",
        "            \"team_name\": team_name,\n",
        "            \"side\": team_side,\n",
        "            \"team_points\": team_points,\n",
        "            \"opp_points\": opp_points,\n",
        "            \"won\": team_points > opp_points,\n",
        "            \"opponent\": opp_team,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return {\n",
        "            \"team_id\": team_id,\n",
        "            \"team_name\": team_name,\n",
        "            \"games_count\": 0,\n",
        "            \"median_score\": None,\n",
        "            \"win_pct\": None,\n",
        "            \"home_games\": 0,\n",
        "            \"home_win_pct\": None,\n",
        "            \"away_games\": 0,\n",
        "            \"away_win_pct\": None,\n",
        "        }, df\n",
        "    \n",
        "    games_count = len(df)\n",
        "    median_score = float(np.median(df[\"team_points\"])) if games_count > 0 else None\n",
        "    win_pct = round(100.0 * df[\"won\"].mean(), 2) if games_count > 0 else None\n",
        "    \n",
        "    # Stretch: home/away splits\n",
        "    home_df = df[df[\"side\"] == \"home\"]\n",
        "    away_df = df[df[\"side\"] == \"away\"]\n",
        "    home_games = len(home_df)\n",
        "    away_games = len(away_df)\n",
        "    home_win_pct = round(100.0 * home_df[\"won\"].mean(), 2) if home_games > 0 else None\n",
        "    away_win_pct = round(100.0 * away_df[\"won\"].mean(), 2) if away_games > 0 else None\n",
        "    \n",
        "    summary = {\n",
        "        \"team_id\": team_id,\n",
        "        \"team_name\": team_name,\n",
        "        \"games_count\": games_count,\n",
        "        \"median_score\": round(median_score, 2) if median_score is not None else None,\n",
        "        \"win_pct\": win_pct,\n",
        "        \"home_games\": home_games,\n",
        "        \"home_win_pct\": home_win_pct,\n",
        "        \"away_games\": away_games,\n",
        "        \"away_win_pct\": away_win_pct,\n",
        "    }\n",
        "    return summary, df\n",
        "\n",
        "def aggregate_to_csv(team_id: int, start_date: str, end_date: str, out_path: str = None):\n",
        "    \"\"\"\n",
        "    High-level convenience:\n",
        "    - fetch games\n",
        "    - compute summary\n",
        "    - write a 1-row CSV of aggregated stats\n",
        "    - also return (summary_dict, per_game_df)\n",
        "    \"\"\"\n",
        "    games = fetch_games_for_team(team_id, start_date, end_date)\n",
        "    summary, df_games = compute_team_aggregates(games, team_id)\n",
        "    # Default path\n",
        "    if out_path is None:\n",
        "        out_path = f\"team_{team_id}_{start_date}_to_{end_date}_summary.csv\"\n",
        "    # Single-row CSV for the spec\n",
        "    pd.DataFrame([summary]).to_csv(out_path, index=False)\n",
        "    return summary, df_games, out_path\n",
        "\n",
        "# Example: user supplies a name or abbreviation (human-friendly)\n",
        "#tid = resolve_team(\"BOS\")              # -> 2 (Boston Celtics)\n",
        "#tid = resolve_team(\"Boston Celtics\") # also works\n",
        "tid = resolve_team(\"bulls\")        # also works\n",
        "\n",
        "# Now call your existing aggregator\n",
        "summary, per_game_df, csv_path = aggregate_to_csv(\n",
        "    tid, start_date=\"2025-01-30\", end_date=\"2025-04-01\"\n",
        ")\n",
        "summary, csv_path\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
